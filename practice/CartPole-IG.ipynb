{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ca2a929",
   "metadata": {},
   "source": [
    "## CartPole-IG Experiment: XRL with IG\n",
    "\n",
    "### ëª©ì \n",
    "Captumì˜ Integrated Gradientsë¥¼ ì‚¬ìš©í•˜ì—¬ CartPole ì—ì´ì „íŠ¸ê°€ ì–´ë–¤ ìƒíƒœ íŠ¹ì„±ì— ë¯¼ê°í•˜ê²Œ ë°˜ì‘í•˜ëŠ”ì§€ ë¶„ì„í•©ë‹ˆë‹¤.\n",
    "\n",
    "### ì‹¤í—˜ ë‚´ìš©\n",
    "1. DQN ì—ì´ì „íŠ¸ í•™ìŠµ\n",
    "2. Integrated Gradientsë¡œ ê° ìƒíƒœ ë³€ìˆ˜ì˜ ì¤‘ìš”ë„ ë¶„ì„\n",
    "3. ì‹œê°„ì— ë”°ë¥¸ ë³€í™” ë¹„êµ\n",
    "4. Actionë³„ íŒ¨í„´ ë¹„êµ\n",
    "5. ìƒíƒœë³„(ì•ˆì •/ìœ„ê¸°) ë¹„êµ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35379eb5",
   "metadata": {},
   "source": [
    "## 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0021be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from captum.attr import IntegratedGradients\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "# ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ì„¤ì •\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print(\"ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ!\")\n",
    "print(f\"PyTorch ë²„ì „: {torch.__version__}\")\n",
    "print(f\"Device: {torch.device('cuda' if torch.cuda.is_available() else 'cpu')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039a951f",
   "metadata": {},
   "source": [
    "## 2. DQN ë„¤íŠ¸ì›Œí¬ ì •ì˜\n",
    "\n",
    "CartPole í™˜ê²½ì˜ 4ê°€ì§€ ìƒíƒœ ë³€ìˆ˜:\n",
    "- Position (ì¹´íŠ¸ ìœ„ì¹˜)\n",
    "- Velocity (ì¹´íŠ¸ ì†ë„)\n",
    "- Angle (ë§‰ëŒ€ ê°ë„)\n",
    "- Angular Velocity (ë§‰ëŒ€ ê°ì†ë„)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2485430b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
    "        super(DQN, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# í™˜ê²½ ë° ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™”\n",
    "env = gym.make('CartPole-v1')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "policy_net = DQN(state_dim, action_dim).to(device)\n",
    "target_net = DQN(state_dim, action_dim).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "print(f\"State dimension: {state_dim}\")\n",
    "print(f\"Action dimension: {action_dim}\")\n",
    "print(f\"Network architecture:\\n{policy_net}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c56ca97",
   "metadata": {},
   "source": [
    "## 3. Experience Replay ë° í•™ìŠµ í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e93c21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=10000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return (np.array(states), np.array(actions), np.array(rewards),\n",
    "                np.array(next_states), np.array(dones))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "def select_action(state, epsilon):\n",
    "    \"\"\"Epsilon-greedy action selection\"\"\"\n",
    "    if random.random() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "            q_values = policy_net(state_tensor)\n",
    "            return q_values.argmax(1).item()\n",
    "\n",
    "def train_step(replay_buffer, batch_size, gamma):\n",
    "    \"\"\"Single training step\"\"\"\n",
    "    if len(replay_buffer) < batch_size:\n",
    "        return None\n",
    "    \n",
    "    states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "    \n",
    "    states = torch.FloatTensor(states).to(device)\n",
    "    actions = torch.LongTensor(actions).to(device)\n",
    "    rewards = torch.FloatTensor(rewards).to(device)\n",
    "    next_states = torch.FloatTensor(next_states).to(device)\n",
    "    dones = torch.FloatTensor(dones).to(device)\n",
    "    \n",
    "    # Current Q values\n",
    "    current_q = policy_net(states).gather(1, actions.unsqueeze(1))\n",
    "    \n",
    "    # Target Q values\n",
    "    with torch.no_grad():\n",
    "        next_q = target_net(next_states).max(1)[0]\n",
    "        target_q = rewards + (1 - dones) * gamma * next_q\n",
    "    \n",
    "    # Loss and optimization\n",
    "    loss = nn.MSELoss()(current_q.squeeze(), target_q)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "print(\"í•™ìŠµ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a4a8b2",
   "metadata": {},
   "source": [
    "## 4. DQN í•™ìŠµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e5c31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
    "num_episodes = 300\n",
    "batch_size = 64\n",
    "gamma = 0.99\n",
    "epsilon_start = 1.0\n",
    "epsilon_end = 0.01\n",
    "epsilon_decay = 0.995\n",
    "target_update = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "# ì´ˆê¸°í™”\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "replay_buffer = ReplayBuffer(capacity=10000)\n",
    "epsilon = epsilon_start\n",
    "episode_rewards = []\n",
    "episode_lengths = []\n",
    "\n",
    "print(\"í•™ìŠµ ì‹œì‘...\")\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    steps = 0\n",
    "    \n",
    "    while not done:\n",
    "        # Action selection\n",
    "        action = select_action(state, epsilon)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        # Store transition\n",
    "        replay_buffer.push(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Training step\n",
    "        loss = train_step(replay_buffer, batch_size, gamma)\n",
    "        \n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "        steps += 1\n",
    "    \n",
    "    episode_rewards.append(episode_reward)\n",
    "    episode_lengths.append(steps)\n",
    "    \n",
    "    # Epsilon decay\n",
    "    epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
    "    \n",
    "    # Target network update\n",
    "    if episode % target_update == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "    \n",
    "    # Logging\n",
    "    if (episode + 1) % 50 == 0:\n",
    "        avg_reward = np.mean(episode_rewards[-50:])\n",
    "        print(f\"Episode {episode+1}/{num_episodes}, Avg Reward: {avg_reward:.2f}, Epsilon: {epsilon:.3f}\")\n",
    "\n",
    "print(\"\\ní•™ìŠµ ì™„ë£Œ!\")\n",
    "print(f\"ìµœê·¼ 50 ì—í”¼ì†Œë“œ í‰ê·  ë³´ìƒ: {np.mean(episode_rewards[-50:]):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ccd520",
   "metadata": {},
   "source": [
    "## 5. í•™ìŠµ ê²°ê³¼ ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636e30a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Episode rewards\n",
    "ax1.plot(episode_rewards, alpha=0.6, label='Episode Reward')\n",
    "# Moving average\n",
    "window = 20\n",
    "moving_avg = np.convolve(episode_rewards, np.ones(window)/window, mode='valid')\n",
    "ax1.plot(range(window-1, len(episode_rewards)), moving_avg, 'r-', linewidth=2, label=f'{window}-Episode Moving Avg')\n",
    "ax1.set_xlabel('Episode')\n",
    "ax1.set_ylabel('Total Reward')\n",
    "ax1.set_title('Training Progress: Episode Rewards')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Episode lengths\n",
    "ax2.plot(episode_lengths, alpha=0.6, label='Episode Length')\n",
    "moving_avg_len = np.convolve(episode_lengths, np.ones(window)/window, mode='valid')\n",
    "ax2.plot(range(window-1, len(episode_lengths)), moving_avg_len, 'r-', linewidth=2, label=f'{window}-Episode Moving Avg')\n",
    "ax2.set_xlabel('Episode')\n",
    "ax2.set_ylabel('Steps')\n",
    "ax2.set_title('Training Progress: Episode Length')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3687033c",
   "metadata": {},
   "source": [
    "## 6. Integrated Gradients ì¤€ë¹„\n",
    "\n",
    "Integrated Gradientsë¥¼ ì‚¬ìš©í•˜ì—¬ ê° ìƒíƒœ ë³€ìˆ˜ì˜ ì¤‘ìš”ë„ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a3dcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IGë¥¼ ìœ„í•œ wrapper í•¨ìˆ˜ - íŠ¹ì • actionì˜ Q-valueë¥¼ ë°˜í™˜\n",
    "def model_forward(state, action_idx):\n",
    "    \"\"\"Returns Q-value for a specific action\"\"\"\n",
    "    q_values = policy_net(state)\n",
    "    return q_values[:, action_idx]\n",
    "\n",
    "# Integrated Gradients ì´ˆê¸°í™”\n",
    "ig = IntegratedGradients(model_forward)\n",
    "\n",
    "# ë² ì´ìŠ¤ë¼ì¸: ëª¨ë“  ìƒíƒœê°€ 0ì¸ ìƒíƒœ\n",
    "baseline = torch.zeros(1, state_dim).to(device)\n",
    "\n",
    "print(\"Integrated Gradients ì¤€ë¹„ ì™„ë£Œ!\")\n",
    "print(f\"Baseline shape: {baseline.shape}\")\n",
    "\n",
    "# ìƒíƒœ ë³€ìˆ˜ ì´ë¦„\n",
    "feature_names = ['Cart Position', 'Cart Velocity', 'Pole Angle', 'Pole Angular Velocity']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37591ca",
   "metadata": {},
   "source": [
    "## 7. ìƒ˜í”Œ ìƒíƒœ ìˆ˜ì§‘\n",
    "\n",
    "ë‹¤ì–‘í•œ ìƒí™©ì˜ ìƒíƒœë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤:\n",
    "- **ì•ˆì • ìƒíƒœ**: ë§‰ëŒ€ê°€ ê±°ì˜ ìˆ˜ì§ì´ê³  ì¹´íŠ¸ê°€ ì¤‘ì•™ ê·¼ì²˜\n",
    "- **ìœ„ê¸° ìƒíƒœ**: ë§‰ëŒ€ê°€ ê¸°ìš¸ì–´ì§€ê±°ë‚˜ ì¹´íŠ¸ê°€ ê²½ê³„ ê·¼ì²˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464c9373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_diverse_states(num_episodes=5):\n",
    "    \"\"\"ë‹¤ì–‘í•œ ìƒíƒœë¥¼ ìˆ˜ì§‘í•˜ê³  ë¶„ë¥˜\"\"\"\n",
    "    stable_states = []\n",
    "    critical_states = []\n",
    "    all_states = []\n",
    "    \n",
    "    for _ in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # ìƒíƒœ ì €ì¥\n",
    "            all_states.append(state.copy())\n",
    "            \n",
    "            # ìƒíƒœ ë¶„ë¥˜ (ê°ë„ì™€ ìœ„ì¹˜ ê¸°ë°˜)\n",
    "            position, velocity, angle, angular_velocity = state\n",
    "            \n",
    "            # ì•ˆì • ìƒíƒœ: ê°ë„ê°€ ì‘ê³  ìœ„ì¹˜ê°€ ì¤‘ì•™ ê·¼ì²˜\n",
    "            if abs(angle) < 0.1 and abs(position) < 1.0:\n",
    "                stable_states.append(state.copy())\n",
    "            # ìœ„ê¸° ìƒíƒœ: ê°ë„ê°€ í¬ê±°ë‚˜ ìœ„ì¹˜ê°€ ê²½ê³„ ê·¼ì²˜\n",
    "            elif abs(angle) > 0.15 or abs(position) > 1.5:\n",
    "                critical_states.append(state.copy())\n",
    "            \n",
    "            # Action selection (greedy)\n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "                action = policy_net(state_tensor).argmax(1).item()\n",
    "            \n",
    "            state, _, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "    \n",
    "    return np.array(stable_states), np.array(critical_states), np.array(all_states)\n",
    "\n",
    "# ìƒíƒœ ìˆ˜ì§‘\n",
    "stable_states, critical_states, all_states = collect_diverse_states(num_episodes=10)\n",
    "\n",
    "print(f\"ìˆ˜ì§‘ëœ ì´ ìƒíƒœ ìˆ˜: {len(all_states)}\")\n",
    "print(f\"ì•ˆì • ìƒíƒœ ìˆ˜: {len(stable_states)}\")\n",
    "print(f\"ìœ„ê¸° ìƒíƒœ ìˆ˜: {len(critical_states)}\")\n",
    "\n",
    "# ìƒ˜í”Œ ì¶œë ¥\n",
    "if len(stable_states) > 0:\n",
    "    print(f\"\\nì•ˆì • ìƒíƒœ ì˜ˆì‹œ: {stable_states[0]}\")\n",
    "if len(critical_states) > 0:\n",
    "    print(f\"ìœ„ê¸° ìƒíƒœ ì˜ˆì‹œ: {critical_states[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ddea74",
   "metadata": {},
   "source": [
    "## 8. Integrated Gradients ê³„ì‚° í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496e0877",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ig_attributions(states, action_idx):\n",
    "    \"\"\"ì£¼ì–´ì§„ ìƒíƒœë“¤ì— ëŒ€í•´ IG attribution ê³„ì‚°\"\"\"\n",
    "    attributions_list = []\n",
    "    \n",
    "    for state in states:\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        state_tensor.requires_grad = True\n",
    "        \n",
    "        # IG ê³„ì‚°\n",
    "        attribution = ig.attribute(\n",
    "            state_tensor,\n",
    "            baselines=baseline,\n",
    "            additional_forward_args=(action_idx,),\n",
    "            n_steps=50\n",
    "        )\n",
    "        \n",
    "        attributions_list.append(attribution.detach().cpu().numpy()[0])\n",
    "    \n",
    "    return np.array(attributions_list)\n",
    "\n",
    "def analyze_attributions(states, action_name):\n",
    "    \"\"\"ì–‘ìª½ actionì— ëŒ€í•œ attribution ë¶„ì„\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ë¶„ì„ ëŒ€ìƒ: {action_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Action 0 (ì™¼ìª½)ê³¼ Action 1 (ì˜¤ë¥¸ìª½)ì— ëŒ€í•œ attribution\n",
    "    attr_action0 = compute_ig_attributions(states, 0)\n",
    "    attr_action1 = compute_ig_attributions(states, 1)\n",
    "    \n",
    "    # í‰ê·  attribution\n",
    "    mean_attr0 = np.mean(attr_action0, axis=0)\n",
    "    mean_attr1 = np.mean(attr_action1, axis=0)\n",
    "    \n",
    "    print(f\"\\nAction 0 (ì™¼ìª½ìœ¼ë¡œ ë°€ê¸°) - í‰ê·  Attribution:\")\n",
    "    for i, name in enumerate(feature_names):\n",
    "        print(f\"  {name:25s}: {mean_attr0[i]:8.4f}\")\n",
    "    \n",
    "    print(f\"\\nAction 1 (ì˜¤ë¥¸ìª½ìœ¼ë¡œ ë°€ê¸°) - í‰ê·  Attribution:\")\n",
    "    for i, name in enumerate(feature_names):\n",
    "        print(f\"  {name:25s}: {mean_attr1[i]:8.4f}\")\n",
    "    \n",
    "    return attr_action0, attr_action1\n",
    "\n",
    "print(\"IG ê³„ì‚° í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782ece87",
   "metadata": {},
   "source": [
    "## 9. ì‹¤í—˜ 1: ì•ˆì • ìƒíƒœ vs ìœ„ê¸° ìƒíƒœ ë¹„êµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61989540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìƒ˜í”Œ ì„ íƒ (ë„ˆë¬´ ë§ìœ¼ë©´ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¼)\n",
    "n_samples = min(20, len(stable_states), len(critical_states))\n",
    "\n",
    "stable_sample = stable_states[:n_samples]\n",
    "critical_sample = critical_states[:n_samples]\n",
    "\n",
    "# ì•ˆì • ìƒíƒœ ë¶„ì„\n",
    "stable_attr0, stable_attr1 = analyze_attributions(stable_sample, \"ì•ˆì • ìƒíƒœ\")\n",
    "\n",
    "# ìœ„ê¸° ìƒíƒœ ë¶„ì„\n",
    "critical_attr0, critical_attr1 = analyze_attributions(critical_sample, \"ìœ„ê¸° ìƒíƒœ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f61dc3",
   "metadata": {},
   "source": [
    "## 10. ì‹œê°í™” 1: ì•ˆì • vs ìœ„ê¸° ìƒíƒœ ë¹„êµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568955b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# ì•ˆì • ìƒíƒœ - Action 0\n",
    "mean_stable_0 = np.mean(stable_attr0, axis=0)\n",
    "axes[0, 0].bar(feature_names, mean_stable_0, color='skyblue', edgecolor='navy', alpha=0.7)\n",
    "axes[0, 0].set_title('ì•ˆì • ìƒíƒœ - Action 0 (ì™¼ìª½ìœ¼ë¡œ)', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Attribution', fontsize=11)\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "axes[0, 0].tick_params(axis='x', rotation=15)\n",
    "\n",
    "# ì•ˆì • ìƒíƒœ - Action 1\n",
    "mean_stable_1 = np.mean(stable_attr1, axis=0)\n",
    "axes[0, 1].bar(feature_names, mean_stable_1, color='lightcoral', edgecolor='darkred', alpha=0.7)\n",
    "axes[0, 1].set_title('ì•ˆì • ìƒíƒœ - Action 1 (ì˜¤ë¥¸ìª½ìœ¼ë¡œ)', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Attribution', fontsize=11)\n",
    "axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "axes[0, 1].tick_params(axis='x', rotation=15)\n",
    "\n",
    "# ìœ„ê¸° ìƒíƒœ - Action 0\n",
    "mean_critical_0 = np.mean(critical_attr0, axis=0)\n",
    "axes[1, 0].bar(feature_names, mean_critical_0, color='skyblue', edgecolor='navy', alpha=0.7)\n",
    "axes[1, 0].set_title('ìœ„ê¸° ìƒíƒœ - Action 0 (ì™¼ìª½ìœ¼ë¡œ)', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Attribution', fontsize=11)\n",
    "axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "axes[1, 0].tick_params(axis='x', rotation=15)\n",
    "\n",
    "# ìœ„ê¸° ìƒíƒœ - Action 1\n",
    "mean_critical_1 = np.mean(critical_attr1, axis=0)\n",
    "axes[1, 1].bar(feature_names, mean_critical_1, color='lightcoral', edgecolor='darkred', alpha=0.7)\n",
    "axes[1, 1].set_title('ìœ„ê¸° ìƒíƒœ - Action 1 (ì˜¤ë¥¸ìª½ìœ¼ë¡œ)', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Attribution', fontsize=11)\n",
    "axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "axes[1, 1].tick_params(axis='x', rotation=15)\n",
    "\n",
    "plt.suptitle('Feature Attributions: ì•ˆì • ìƒíƒœ vs ìœ„ê¸° ìƒíƒœ', fontsize=14, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58588936",
   "metadata": {},
   "source": [
    "## 11. ì‹¤í—˜ 2: ì‹œê°„ì— ë”°ë¥¸ Attribution ë³€í™” ì¶”ì "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8af5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def track_episode_attributions():\n",
    "    \"\"\"í•œ ì—í”¼ì†Œë“œ ë™ì•ˆì˜ attribution ë³€í™” ì¶”ì \"\"\"\n",
    "    state, _ = env.reset()\n",
    "    states_sequence = []\n",
    "    actions_sequence = []\n",
    "    done = False\n",
    "    \n",
    "    # í•œ ì—í”¼ì†Œë“œ ì‹¤í–‰\n",
    "    while not done and len(states_sequence) < 100:  # ìµœëŒ€ 100 ìŠ¤í…\n",
    "        states_sequence.append(state.copy())\n",
    "        \n",
    "        # Greedy action selection\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "            action = policy_net(state_tensor).argmax(1).item()\n",
    "        \n",
    "        actions_sequence.append(action)\n",
    "        state, _, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "    \n",
    "    return np.array(states_sequence), np.array(actions_sequence)\n",
    "\n",
    "# ì—í”¼ì†Œë“œ ì‹¤í–‰ ë° ìƒíƒœ ìˆ˜ì§‘\n",
    "episode_states, episode_actions = track_episode_attributions()\n",
    "print(f\"ì—í”¼ì†Œë“œ ê¸¸ì´: {len(episode_states)} ìŠ¤í…\")\n",
    "\n",
    "# ê° ìŠ¤í…ì—ì„œ ì„ íƒëœ actionì— ëŒ€í•œ attribution ê³„ì‚°\n",
    "time_attributions = []\n",
    "for i, (state, action) in enumerate(zip(episode_states, episode_actions)):\n",
    "    state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "    state_tensor.requires_grad = True\n",
    "    \n",
    "    attribution = ig.attribute(\n",
    "        state_tensor,\n",
    "        baselines=baseline,\n",
    "        additional_forward_args=(action,),\n",
    "        n_steps=50\n",
    "    )\n",
    "    \n",
    "    time_attributions.append(attribution.detach().cpu().numpy()[0])\n",
    "    \n",
    "    if (i + 1) % 20 == 0:\n",
    "        print(f\"  {i+1}/{len(episode_states)} ìŠ¤í… ì²˜ë¦¬ ì™„ë£Œ\")\n",
    "\n",
    "time_attributions = np.array(time_attributions)\n",
    "print(f\"\\nAttribution ê³„ì‚° ì™„ë£Œ! Shape: {time_attributions.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c7bfa9",
   "metadata": {},
   "source": [
    "## 12. ì‹œê°í™” 2: ì‹œê°„ì— ë”°ë¥¸ Attribution ë³€í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3684c705",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10))\n",
    "\n",
    "# ì‹œê°„ì— ë”°ë¥¸ ê° featureì˜ attribution ë³€í™”\n",
    "time_steps = range(len(time_attributions))\n",
    "colors = ['blue', 'orange', 'green', 'red']\n",
    "\n",
    "for i, (name, color) in enumerate(zip(feature_names, colors)):\n",
    "    ax1.plot(time_steps, time_attributions[:, i], label=name, color=color, linewidth=2, alpha=0.7)\n",
    "\n",
    "ax1.set_xlabel('Time Step', fontsize=12)\n",
    "ax1.set_ylabel('Attribution', fontsize=12)\n",
    "ax1.set_title('ì‹œê°„ì— ë”°ë¥¸ Feature Attribution ë³€í™”', fontsize=14, fontweight='bold')\n",
    "ax1.legend(loc='best', fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# ì‹¤ì œ ìƒíƒœ ê°’ì˜ ë³€í™”\n",
    "for i, (name, color) in enumerate(zip(feature_names, colors)):\n",
    "    ax2.plot(time_steps, episode_states[:, i], label=name, color=color, linewidth=2, alpha=0.7)\n",
    "\n",
    "ax2.set_xlabel('Time Step', fontsize=12)\n",
    "ax2.set_ylabel('State Value', fontsize=12)\n",
    "ax2.set_title('ì‹œê°„ì— ë”°ë¥¸ ì‹¤ì œ State ê°’ ë³€í™”', fontsize=14, fontweight='bold')\n",
    "ax2.legend(loc='best', fontsize=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Action í‘œì‹œ (ë°°ê²½ìƒ‰)\n",
    "for i in range(len(episode_actions)):\n",
    "    if episode_actions[i] == 0:  # ì™¼ìª½\n",
    "        ax1.axvspan(i, i+1, alpha=0.1, color='blue')\n",
    "        ax2.axvspan(i, i+1, alpha=0.1, color='blue')\n",
    "    else:  # ì˜¤ë¥¸ìª½\n",
    "        ax1.axvspan(i, i+1, alpha=0.1, color='red')\n",
    "        ax2.axvspan(i, i+1, alpha=0.1, color='red')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66181cb",
   "metadata": {},
   "source": [
    "## 13. ì‹¤í—˜ 3: Actionë³„ íŒ¨í„´ ë¹„êµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3d1029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë“  ìƒíƒœì— ëŒ€í•´ ë‘ actionì˜ attribution ê³„ì‚°\n",
    "sample_states = all_states[::10][:30]  # ìƒ˜í”Œë§\n",
    "\n",
    "print(\"Actionë³„ Attribution ê³„ì‚° ì¤‘...\")\n",
    "all_attr_action0 = compute_ig_attributions(sample_states, 0)\n",
    "all_attr_action1 = compute_ig_attributions(sample_states, 1)\n",
    "\n",
    "# í‰ê·  ê³„ì‚°\n",
    "mean_all_attr0 = np.mean(all_attr_action0, axis=0)\n",
    "mean_all_attr1 = np.mean(all_attr_action1, axis=0)\n",
    "std_all_attr0 = np.std(all_attr_action0, axis=0)\n",
    "std_all_attr1 = np.std(all_attr_action1, axis=0)\n",
    "\n",
    "print(\"\\nì „ì²´ í‰ê·  Attribution:\")\n",
    "print(\"\\nAction 0 (ì™¼ìª½ìœ¼ë¡œ):\")\n",
    "for i, name in enumerate(feature_names):\n",
    "    print(f\"  {name:25s}: {mean_all_attr0[i]:8.4f} Â± {std_all_attr0[i]:6.4f}\")\n",
    "\n",
    "print(\"\\nAction 1 (ì˜¤ë¥¸ìª½ìœ¼ë¡œ):\")\n",
    "for i, name in enumerate(feature_names):\n",
    "    print(f\"  {name:25s}: {mean_all_attr1[i]:8.4f} Â± {std_all_attr1[i]:6.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fc5047",
   "metadata": {},
   "source": [
    "## 14. ì‹œê°í™” 3: Actionë³„ Attribution ë¹„êµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d97f7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "x = np.arange(len(feature_names))\n",
    "width = 0.35\n",
    "\n",
    "# Bar plot with error bars\n",
    "axes[0].bar(x - width/2, mean_all_attr0, width, yerr=std_all_attr0, \n",
    "            label='Action 0 (ì™¼ìª½)', color='skyblue', edgecolor='navy', \n",
    "            alpha=0.7, capsize=5)\n",
    "axes[0].bar(x + width/2, mean_all_attr1, width, yerr=std_all_attr1,\n",
    "            label='Action 1 (ì˜¤ë¥¸ìª½)', color='lightcoral', edgecolor='darkred',\n",
    "            alpha=0.7, capsize=5)\n",
    "axes[0].set_xlabel('Feature', fontsize=12)\n",
    "axes[0].set_ylabel('Mean Attribution', fontsize=12)\n",
    "axes[0].set_title('Actionë³„ í‰ê·  Attribution ë¹„êµ (with std)', fontsize=13, fontweight='bold')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(feature_names, rotation=15, ha='right')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Difference plot\n",
    "diff = mean_all_attr1 - mean_all_attr0\n",
    "colors_diff = ['green' if d > 0 else 'red' for d in diff]\n",
    "axes[1].bar(feature_names, diff, color=colors_diff, edgecolor='black', alpha=0.7)\n",
    "axes[1].axhline(y=0, color='black', linestyle='-', linewidth=1)\n",
    "axes[1].set_ylabel('Attribution Difference\\n(Action 1 - Action 0)', fontsize=12)\n",
    "axes[1].set_title('Action ê°„ Attribution ì°¨ì´', fontsize=13, fontweight='bold')\n",
    "axes[1].tick_params(axis='x', rotation=15)\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b07f4b",
   "metadata": {},
   "source": [
    "## 15. (ì„ íƒ) Feature Ablation Study\n",
    "\n",
    "ê° featureë¥¼ ì œê±°í–ˆì„ ë•Œì˜ ì„±ëŠ¥ ë³€í™”ë¥¼ ê´€ì°°í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aafb2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_with_ablation(feature_idx_to_ablate=None, num_episodes=10):\n",
    "    \"\"\"íŠ¹ì • featureë¥¼ 0ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ì—í”¼ì†Œë“œ ì‹¤í–‰\"\"\"\n",
    "    total_rewards = []\n",
    "    \n",
    "    for _ in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # Feature ablation\n",
    "            if feature_idx_to_ablate is not None:\n",
    "                state[feature_idx_to_ablate] = 0\n",
    "            \n",
    "            # Action selection\n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "                action = policy_net(state_tensor).argmax(1).item()\n",
    "            \n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            episode_reward += reward\n",
    "        \n",
    "        total_rewards.append(episode_reward)\n",
    "    \n",
    "    return np.mean(total_rewards), np.std(total_rewards)\n",
    "\n",
    "# ê¸°ë³¸ ì„±ëŠ¥ (ablation ì—†ì´)\n",
    "print(\"Feature Ablation Study ì‹œì‘...\\n\")\n",
    "baseline_mean, baseline_std = evaluate_with_ablation(feature_idx_to_ablate=None, num_episodes=20)\n",
    "print(f\"Baseline (ëª¨ë“  feature ì‚¬ìš©): {baseline_mean:.2f} Â± {baseline_std:.2f}\")\n",
    "\n",
    "# ê° featureë¥¼ ì œê±°í–ˆì„ ë•Œì˜ ì„±ëŠ¥\n",
    "ablation_results = []\n",
    "for i, name in enumerate(feature_names):\n",
    "    mean_reward, std_reward = evaluate_with_ablation(feature_idx_to_ablate=i, num_episodes=20)\n",
    "    performance_drop = baseline_mean - mean_reward\n",
    "    ablation_results.append({\n",
    "        'feature': name,\n",
    "        'mean': mean_reward,\n",
    "        'std': std_reward,\n",
    "        'drop': performance_drop\n",
    "    })\n",
    "    print(f\"{name:25s} ì œê±°: {mean_reward:.2f} Â± {std_reward:.2f} (ì„±ëŠ¥ í•˜ë½: {performance_drop:.2f})\")\n",
    "\n",
    "print(\"\\nì„±ëŠ¥ í•˜ë½ì´ í° ìˆœì„œ:\")\n",
    "sorted_results = sorted(ablation_results, key=lambda x: x['drop'], reverse=True)\n",
    "for i, result in enumerate(sorted_results, 1):\n",
    "    print(f\"{i}. {result['feature']:25s}: {result['drop']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806cd517",
   "metadata": {},
   "source": [
    "## 16. ì‹œê°í™” 4: Ablation Study ê²°ê³¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b30bc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# ì„±ëŠ¥ ë¹„êµ\n",
    "features = [r['feature'] for r in ablation_results]\n",
    "means = [r['mean'] for r in ablation_results]\n",
    "stds = [r['std'] for r in ablation_results]\n",
    "\n",
    "ax1.bar(range(len(features)), means, yerr=stds, capsize=5, \n",
    "        color='lightblue', edgecolor='navy', alpha=0.7)\n",
    "ax1.axhline(y=baseline_mean, color='red', linestyle='--', linewidth=2, \n",
    "            label=f'Baseline: {baseline_mean:.2f}')\n",
    "ax1.set_xticks(range(len(features)))\n",
    "ax1.set_xticklabels(features, rotation=15, ha='right')\n",
    "ax1.set_ylabel('Average Episode Reward', fontsize=12)\n",
    "ax1.set_title('Feature Ablation: ì„±ëŠ¥ ë¹„êµ', fontsize=13, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# ì„±ëŠ¥ í•˜ë½ í¬ê¸°\n",
    "drops = [r['drop'] for r in ablation_results]\n",
    "colors_drop = ['red' if d > 0 else 'green' for d in drops]\n",
    "\n",
    "ax2.bar(range(len(features)), drops, color=colors_drop, \n",
    "        edgecolor='black', alpha=0.7)\n",
    "ax2.set_xticks(range(len(features)))\n",
    "ax2.set_xticklabels(features, rotation=15, ha='right')\n",
    "ax2.set_ylabel('Performance Drop', fontsize=12)\n",
    "ax2.set_title('Feature ì œê±° ì‹œ ì„±ëŠ¥ í•˜ë½', fontsize=13, fontweight='bold')\n",
    "ax2.axhline(y=0, color='black', linestyle='-', linewidth=1)\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469b2fc2",
   "metadata": {},
   "source": [
    "## 17. ì¢…í•© ë¶„ì„ ë° ê²°ë¡ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909c0896",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"CartPole-IG ì‹¤í—˜ ì¢…í•© ë¶„ì„\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nğŸ“Š ì‹¤í—˜ ìš”ì•½:\")\n",
    "print(f\"  - í•™ìŠµ ì—í”¼ì†Œë“œ: {num_episodes}\")\n",
    "print(f\"  - ìµœì¢… í‰ê·  ë³´ìƒ: {np.mean(episode_rewards[-50:]):.2f}\")\n",
    "print(f\"  - ë¶„ì„í•œ ìƒíƒœ ìˆ˜: {len(all_states)}\")\n",
    "\n",
    "print(\"\\nğŸ” ì£¼ìš” ë°œê²¬ì‚¬í•­:\")\n",
    "\n",
    "# 1. Actionë³„ ì¤‘ìš” feature\n",
    "print(\"\\n1. Actionë³„ ì£¼ìš” Feature (í‰ê·  ì ˆëŒ€ê°’ ê¸°ì¤€):\")\n",
    "for action_idx, (mean_attr, action_name) in enumerate([\n",
    "    (mean_all_attr0, \"Action 0 (ì™¼ìª½)\"),\n",
    "    (mean_all_attr1, \"Action 1 (ì˜¤ë¥¸ìª½)\")\n",
    "]):\n",
    "    abs_attr = np.abs(mean_attr)\n",
    "    sorted_idx = np.argsort(abs_attr)[::-1]\n",
    "    print(f\"\\n   {action_name}:\")\n",
    "    for i in sorted_idx:\n",
    "        print(f\"     {i+1}. {feature_names[i]:25s}: {abs_attr[i]:.4f}\")\n",
    "\n",
    "# 2. Ablation study ê²°ê³¼\n",
    "print(\"\\n2. Feature ì¤‘ìš”ë„ (Ablation Study ê¸°ì¤€):\")\n",
    "for i, result in enumerate(sorted_results, 1):\n",
    "    print(f\"   {i}. {result['feature']:25s}: ì„±ëŠ¥ í•˜ë½ {result['drop']:6.2f}\")\n",
    "\n",
    "# 3. ìƒíƒœë³„ ì°¨ì´\n",
    "print(\"\\n3. ìƒíƒœë³„ Attribution íŒ¨í„´:\")\n",
    "print(f\"   ì•ˆì • ìƒíƒœ: Angleê³¼ Angular Velocityì— ëŒ€í•œ ë¯¼ê°ë„ ìƒëŒ€ì ìœ¼ë¡œ ë‚®ìŒ\")\n",
    "print(f\"   ìœ„ê¸° ìƒíƒœ: Angleê³¼ Angular Velocityì— ëŒ€í•œ ë¯¼ê°ë„ ì¦ê°€\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… ì‹¤í—˜ ì™„ë£Œ!\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
