{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "D71frhDjc9Ce",
      "metadata": {
        "id": "D71frhDjc9Ce"
      },
      "source": [
        "# CIFAR-10 GAP Sweep (GoogLeNet, ResNet50, VGG19)\n",
        "\n",
        "Colab-ready notebook for probing how different output sizes of the final Global Average Pooling layer affect performance on CIFAR-10. Each model is trained briefly so you can compare validation accuracy across pooling configurations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OEDN9jLhc9Ch",
      "metadata": {
        "id": "OEDN9jLhc9Ch"
      },
      "source": [
        "## Setup\n",
        "Run the next cell to import packages and configure lightweight hyperparameters. If you're in Colab, enable GPU acceleration via *Runtime → Change runtime type → GPU* first.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ud-RlgFsc9Ch",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ud-RlgFsc9Ch",
        "outputId": "c2416320-5f0e-4fd0-efe5-ec265a7e7a1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch version: 2.8.0+cu126\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "from collections import defaultdict\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, Iterable, List, Optional\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, models, transforms\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b84fe385",
      "metadata": {
        "id": "b84fe385"
      },
      "outputs": [],
      "source": [
        "def get_device() -> torch.device:\n",
        "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "-mZIQzUoc9Ci",
      "metadata": {
        "id": "-mZIQzUoc9Ci"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class ExperimentConfig:\n",
        "    model_name: str\n",
        "    pool_size: Optional[int]\n",
        "    epochs: int = 2\n",
        "    lr: float = 0.01\n",
        "    weight_decay: float = 5e-4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "lT9ruWoIc9Cj",
      "metadata": {
        "id": "lT9ruWoIc9Cj"
      },
      "outputs": [],
      "source": [
        "def _infer_flatten_dim(model: nn.Module, classifier_attr: str, input_size: int = 224) -> int:\n",
        "    classifier = getattr(model, classifier_attr)\n",
        "    setattr(model, classifier_attr, nn.Identity())\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        dummy_input = torch.zeros(1, 3, input_size, input_size)\n",
        "        features = model(dummy_input)\n",
        "    setattr(model, classifier_attr, classifier)\n",
        "    if was_training:\n",
        "        model.train()\n",
        "    return features.shape[1]\n",
        "\n",
        "\n",
        "def build_model(name: str, pool_size: Optional[int], num_classes: int) -> nn.Module:\n",
        "    name = name.lower()\n",
        "    if name == \"resnet50\":\n",
        "        model = models.resnet50(weights=None)\n",
        "        if pool_size is None:\n",
        "            model.avgpool = nn.Identity()\n",
        "            in_features = _infer_flatten_dim(model, \"fc\")\n",
        "            model.fc = nn.Linear(in_features, num_classes)\n",
        "        else:\n",
        "            model.avgpool = nn.AdaptiveAvgPool2d((pool_size, pool_size))\n",
        "            in_features = model.fc.in_features * pool_size * pool_size\n",
        "            model.fc = nn.Linear(in_features, num_classes)\n",
        "    elif name in {\"googlenet\", \"inceptionv1\", \"inception_v1\"}:\n",
        "        model = models.googlenet(weights=None, aux_logits=False)\n",
        "        if pool_size is None:\n",
        "            model.avgpool = nn.Identity()\n",
        "            in_features = _infer_flatten_dim(model, \"fc\")\n",
        "            model.fc = nn.Linear(in_features, num_classes)\n",
        "        else:\n",
        "            model.avgpool = nn.AdaptiveAvgPool2d((pool_size, pool_size))\n",
        "            in_features = model.fc.in_features * pool_size * pool_size\n",
        "            model.fc = nn.Linear(in_features, num_classes)\n",
        "    elif name == \"vgg19\":\n",
        "        model = models.vgg19(weights=None)\n",
        "        if pool_size is None:\n",
        "            model.avgpool = nn.Identity()\n",
        "            in_features = _infer_flatten_dim(model, \"classifier\")\n",
        "        else:\n",
        "            model.avgpool = nn.AdaptiveAvgPool2d((pool_size, pool_size))\n",
        "            in_features = 512 * pool_size * pool_size\n",
        "        classifier = nn.Sequential(\n",
        "            nn.Linear(in_features, 1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=0.3),\n",
        "            nn.Linear(1024, num_classes),\n",
        "        )\n",
        "        model.classifier = classifier\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported model name: {name}\")\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "p_IRRBMIc9Cj",
      "metadata": {
        "id": "p_IRRBMIc9Cj"
      },
      "outputs": [],
      "source": [
        "def get_dataloaders(batch_size: int = 128, num_workers: int = 2) -> Dict[str, DataLoader]:\n",
        "    normalize = transforms.Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2470, 0.2435, 0.2616))\n",
        "\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.Resize(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        normalize,\n",
        "    ])\n",
        "\n",
        "    transform_val = transforms.Compose([\n",
        "        transforms.Resize(224),\n",
        "        transforms.ToTensor(),\n",
        "        normalize,\n",
        "    ])\n",
        "\n",
        "    train_dataset = datasets.CIFAR10(root=\"./data\", train=True, transform=transform_train, download=True)\n",
        "    val_dataset = datasets.CIFAR10(root=\"./data\", train=False, transform=transform_val, download=True)\n",
        "\n",
        "    loaders = {\n",
        "        \"train\": DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers),\n",
        "        \"val\": DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers),\n",
        "    }\n",
        "    return loaders\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "n8BhgLuSc9Ck",
      "metadata": {
        "id": "n8BhgLuSc9Ck"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(\n",
        "    model: nn.Module,\n",
        "    loader: DataLoader,\n",
        "    criterion: nn.Module,\n",
        "    optimizer: optim.Optimizer,\n",
        "    device: torch.device,\n",
        ") -> Dict[str, float]:\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for inputs, targets in loader:\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, preds = outputs.max(1)\n",
        "        correct += preds.eq(targets).sum().item()\n",
        "        total += targets.size(0)\n",
        "\n",
        "    return {\"loss\": running_loss / total, \"acc\": correct / total}\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(\n",
        "    model: nn.Module,\n",
        "    loader: DataLoader,\n",
        "    criterion: nn.Module,\n",
        "    device: torch.device,\n",
        ") -> Dict[str, float]:\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for inputs, targets in loader:\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, preds = outputs.max(1)\n",
        "        correct += preds.eq(targets).sum().item()\n",
        "        total += targets.size(0)\n",
        "\n",
        "    return {\"loss\": running_loss / total, \"acc\": correct / total}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ahTIzvV4c9Ck",
      "metadata": {
        "id": "ahTIzvV4c9Ck"
      },
      "outputs": [],
      "source": [
        "def run_experiment(config: ExperimentConfig, loaders: Dict[str, DataLoader]) -> Dict[str, float]:\n",
        "    device = get_device()\n",
        "    model = build_model(config.model_name, config.pool_size, num_classes=10)\n",
        "    model.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=config.lr, momentum=0.9, weight_decay=config.weight_decay)\n",
        "\n",
        "    history = defaultdict(list)\n",
        "    for epoch in range(config.epochs):\n",
        "        start_time = time.time()\n",
        "        train_metrics = train_one_epoch(model, loaders[\"train\"], criterion, optimizer, device)\n",
        "        val_metrics = evaluate(model, loaders[\"val\"], criterion, device)\n",
        "        epoch_time = time.time() - start_time\n",
        "\n",
        "        history[\"train_loss\"].append(train_metrics[\"loss\"])\n",
        "        history[\"train_acc\"].append(train_metrics[\"acc\"])\n",
        "        history[\"val_loss\"].append(val_metrics[\"loss\"])\n",
        "        history[\"val_acc\"].append(val_metrics[\"acc\"])\n",
        "\n",
        "        pool_label = config.pool_size if config.pool_size is not None else \"none\"\n",
        "        print(\n",
        "            f\"[{config.model_name} | pool={pool_label}] \"\n",
        "            f\"Epoch {epoch + 1}/{config.epochs} \"\n",
        "            f\"train_loss={train_metrics['loss']:.4f} \"\n",
        "            f\"train_acc={train_metrics['acc']*100:.2f}% \"\n",
        "            f\"val_loss={val_metrics['loss']:.4f} \"\n",
        "            f\"val_acc={val_metrics['acc']*100:.2f}% \"\n",
        "            f\"time={epoch_time:.1f}s\"\n",
        "        )\n",
        "\n",
        "    final_metrics = {\n",
        "        \"train_loss\": history[\"train_loss\"][-1],\n",
        "        \"train_acc\": history[\"train_acc\"][-1],\n",
        "        \"val_loss\": history[\"val_loss\"][-1],\n",
        "        \"val_acc\": history[\"val_acc\"][-1],\n",
        "    }\n",
        "    return final_metrics\n",
        "\n",
        "\n",
        "def grid_search(\n",
        "    models_to_try: Iterable[str], pool_sizes: Iterable[Optional[int]], epochs: int = 2\n",
        ") -> List[Dict[str, object]]:\n",
        "    loaders = get_dataloaders()\n",
        "    results = []\n",
        "    for name in models_to_try:\n",
        "        for pool_size in pool_sizes:\n",
        "            cfg = ExperimentConfig(model_name=name, pool_size=pool_size, epochs=epochs)\n",
        "            metrics = run_experiment(cfg, loaders)\n",
        "            results.append(\n",
        "                {\n",
        "                    \"model\": name,\n",
        "                    \"pool_size\": pool_size if pool_size is not None else \"none\",\n",
        "                    \"val_acc\": metrics[\"val_acc\"],\n",
        "                    \"val_loss\": metrics[\"val_loss\"],\n",
        "                    \"train_acc\": metrics[\"train_acc\"],\n",
        "                    \"train_loss\": metrics[\"train_loss\"],\n",
        "                }\n",
        "            )\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ue3CeDmoc9Ck",
      "metadata": {
        "id": "Ue3CeDmoc9Ck"
      },
      "source": [
        "## Run the sweep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Z3U1wljWc9Cl",
      "metadata": {
        "id": "Z3U1wljWc9Cl"
      },
      "outputs": [],
      "source": [
        "MODELS = [\"googlenet\", \"resnet50\", \"vgg19\"]\n",
        "POOL_SIZES = [1]  # None을 사용하면 GAP을 제거\n",
        "EPOCHS = 5\n",
        "\n",
        "summary = grid_search(MODELS, POOL_SIZES, epochs=EPOCHS)\n",
        "summary\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tipqwpDgm_cB",
      "metadata": {
        "id": "tipqwpDgm_cB"
      },
      "source": [
        "### 실험 1 - GAP None\n",
        "\n",
        "```\n",
        "[googlenet | pool=none] Epoch 1/5 train_loss=4.3566 train_acc=17.85% val_loss=1.9445 val_acc=31.10% time=55.2s\n",
        "[googlenet | pool=none] Epoch 2/5 train_loss=1.8909 train_acc=37.74% val_loss=1.6542 val_acc=43.88% time=54.6s\n",
        "[googlenet | pool=none] Epoch 3/5 train_loss=1.5450 train_acc=45.46% val_loss=1.4195 val_acc=50.15% time=54.7s\n",
        "[googlenet | pool=none] Epoch 4/5 train_loss=1.3953 train_acc=50.32% val_loss=1.3109 val_acc=53.19% time=54.8s\n",
        "[googlenet | pool=none] Epoch 5/5 train_loss=1.2819 train_acc=54.06% val_loss=1.2942 val_acc=53.61% time=54.9s\n",
        "[resnet50 | pool=none] Epoch 1/5 train_loss=7.2307 train_acc=19.30% val_loss=1.9333 val_acc=30.84% time=77.2s\n",
        "[resnet50 | pool=none] Epoch 2/5 train_loss=1.7646 train_acc=36.91% val_loss=1.6628 val_acc=43.54% time=77.1s\n",
        "[resnet50 | pool=none] Epoch 3/5 train_loss=1.5644 train_acc=43.76% val_loss=1.4947 val_acc=45.80% time=77.1s\n",
        "[resnet50 | pool=none] Epoch 4/5 train_loss=1.4672 train_acc=47.34% val_loss=1.4114 val_acc=49.24% time=77.1s\n",
        "[resnet50 | pool=none] Epoch 5/5 train_loss=1.3683 train_acc=50.91% val_loss=1.7889 val_acc=50.62% time=77.1s\n",
        "[vgg19 | pool=none] Epoch 1/5 train_loss=1.7766 train_acc=34.40% val_loss=1.3715 val_acc=50.26% time=115.6s\n",
        "[vgg19 | pool=none] Epoch 2/5 train_loss=1.2519 train_acc=55.40% val_loss=1.1114 val_acc=60.45% time=115.7s\n",
        "[vgg19 | pool=none] Epoch 3/5 train_loss=0.9521 train_acc=66.99% val_loss=0.9195 val_acc=68.14% time=115.7s\n",
        "[vgg19 | pool=none] Epoch 4/5 train_loss=0.7523 train_acc=74.11% val_loss=0.7343 val_acc=74.30% time=115.7s\n",
        "[vgg19 | pool=none] Epoch 5/5 train_loss=0.6300 train_acc=78.14% val_loss=0.6748 val_acc=77.27% time=115.8s\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EM9OZvEOnNgS",
      "metadata": {
        "id": "EM9OZvEOnNgS"
      },
      "source": [
        "### 실험 2 - GAP (pool: 1)\n",
        "\n",
        "```\n",
        "[googlenet | pool=1] Epoch 1/5 train_loss=1.4601 train_acc=46.31% val_loss=1.2282 val_acc=55.69% time=53.1s\n",
        "[googlenet | pool=1] Epoch 2/5 train_loss=0.9547 train_acc=66.11% val_loss=1.2028 val_acc=59.15% time=52.9s\n",
        "[googlenet | pool=1] Epoch 3/5 train_loss=0.7161 train_acc=74.77% val_loss=0.7434 val_acc=73.68% time=52.9s\n",
        "[googlenet | pool=1] Epoch 4/5 train_loss=0.5712 train_acc=80.15% val_loss=0.6181 val_acc=78.16% time=52.8s\n",
        "[googlenet | pool=1] Epoch 5/5 train_loss=0.4804 train_acc=83.29% val_loss=0.6487 val_acc=78.60% time=53.5s\n",
        "[resnet50 | pool=1] Epoch 1/5 train_loss=1.9458 train_acc=30.72% val_loss=1.8685 val_acc=41.23% time=77.5s\n",
        "[resnet50 | pool=1] Epoch 2/5 train_loss=1.3987 train_acc=48.98% val_loss=1.7754 val_acc=44.00% time=77.4s\n",
        "[resnet50 | pool=1] Epoch 3/5 train_loss=1.1520 train_acc=58.29% val_loss=1.0552 val_acc=61.25% time=77.3s\n",
        "[resnet50 | pool=1] Epoch 4/5 train_loss=0.9158 train_acc=67.53% val_loss=0.9634 val_acc=66.03% time=77.5s\n",
        "[resnet50 | pool=1] Epoch 5/5 train_loss=0.7634 train_acc=73.05% val_loss=0.8352 val_acc=72.08% time=77.5s\n",
        "[vgg19 | pool=1] Epoch 1/5 train_loss=2.2234 train_acc=15.02% val_loss=2.0774 val_acc=20.49% time=114.8s\n",
        "[vgg19 | pool=1] Epoch 2/5 train_loss=1.9098 train_acc=27.21% val_loss=1.7287 val_acc=34.79% time=114.8s\n",
        "[vgg19 | pool=1] Epoch 3/5 train_loss=1.6704 train_acc=36.96% val_loss=1.5463 val_acc=41.73% time=114.9s\n",
        "[vgg19 | pool=1] Epoch 4/5 train_loss=1.4952 train_acc=44.35% val_loss=1.4130 val_acc=47.12% time=114.8s\n",
        "[vgg19 | pool=1] Epoch 5/5 train_loss=1.3083 train_acc=52.27% val_loss=1.1218 val_acc=59.15% time=114.8s\n",
        "```\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
